{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e17096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import copy\n",
    "\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa9ce417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdaaa6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_loader import MIMICDataset,import_data\n",
    "from src.utils import setup_logger\n",
    "from src.training.training_nn import *\n",
    "from src.models.models import ODERNN\n",
    "from src.utils import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10d61022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glc_transform(x):\n",
    "    x = x.copy()\n",
    "    x[x > 0] = np.log(x[x > 0]) - np.log(140)\n",
    "    return x\n",
    "\n",
    "def glc_invtransform(x):\n",
    "    x = x.copy()\n",
    "    x = np.exp(x + np.log(140))\n",
    "    return x\n",
    "\n",
    "ginv = glc_invtransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "059801f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluding: 200024\n",
      "excluding: 200586\n",
      "excluding: 201104\n",
      "excluding: 201173\n",
      "excluding: 201605\n",
      "excluding: 203383\n",
      "excluding: 205463\n",
      "excluding: 205830\n",
      "excluding: 206288\n",
      "excluding: 206986\n",
      "excluding: 208691\n",
      "excluding: 208937\n",
      "excluding: 212690\n",
      "excluding: 212925\n",
      "excluding: 214617\n",
      "excluding: 215027\n",
      "excluding: 215250\n",
      "excluding: 216192\n",
      "excluding: 216504\n",
      "excluding: 216867\n",
      "excluding: 217645\n",
      "excluding: 218713\n",
      "excluding: 218874\n",
      "excluding: 219153\n",
      "excluding: 219305\n",
      "excluding: 220024\n",
      "excluding: 220671\n",
      "excluding: 220704\n",
      "excluding: 221494\n",
      "excluding: 221514\n",
      "excluding: 221574\n",
      "excluding: 221812\n",
      "excluding: 222096\n",
      "excluding: 223717\n",
      "excluding: 224428\n",
      "excluding: 226283\n",
      "excluding: 227040\n",
      "excluding: 227991\n",
      "excluding: 228262\n",
      "excluding: 229133\n",
      "excluding: 229142\n",
      "excluding: 229929\n",
      "excluding: 230012\n",
      "excluding: 230384\n",
      "excluding: 231775\n",
      "excluding: 232042\n",
      "excluding: 232582\n",
      "excluding: 233553\n",
      "excluding: 234981\n",
      "excluding: 235824\n",
      "excluding: 236999\n",
      "excluding: 237956\n",
      "excluding: 238662\n",
      "excluding: 240029\n",
      "excluding: 240043\n",
      "excluding: 241162\n",
      "excluding: 241858\n",
      "excluding: 245283\n",
      "excluding: 245481\n",
      "excluding: 245797\n",
      "excluding: 246509\n",
      "excluding: 247191\n",
      "excluding: 247831\n",
      "excluding: 249809\n",
      "excluding: 250329\n",
      "excluding: 251008\n",
      "excluding: 252314\n",
      "excluding: 252591\n",
      "excluding: 253402\n",
      "excluding: 254345\n",
      "excluding: 255499\n",
      "excluding: 257353\n",
      "excluding: 258717\n",
      "excluding: 259024\n",
      "excluding: 259870\n",
      "excluding: 260932\n",
      "excluding: 261264\n",
      "excluding: 262078\n",
      "excluding: 262940\n",
      "excluding: 263133\n",
      "excluding: 263642\n",
      "excluding: 264048\n",
      "excluding: 264655\n",
      "excluding: 264914\n",
      "excluding: 265069\n",
      "excluding: 266377\n",
      "excluding: 267367\n",
      "excluding: 269649\n",
      "excluding: 271127\n",
      "excluding: 272414\n",
      "excluding: 272903\n",
      "excluding: 273102\n",
      "excluding: 275175\n",
      "excluding: 275192\n",
      "excluding: 277137\n",
      "excluding: 277327\n",
      "excluding: 277477\n",
      "excluding: 277949\n",
      "excluding: 278500\n",
      "excluding: 278855\n",
      "excluding: 279337\n",
      "excluding: 279937\n",
      "excluding: 280799\n",
      "excluding: 282423\n",
      "excluding: 282997\n",
      "excluding: 284250\n",
      "excluding: 285213\n",
      "excluding: 285482\n",
      "excluding: 285794\n",
      "excluding: 285846\n",
      "excluding: 286788\n",
      "excluding: 287616\n",
      "excluding: 289705\n",
      "excluding: 290296\n",
      "excluding: 291068\n",
      "excluding: 291475\n",
      "excluding: 291578\n",
      "excluding: 292296\n",
      "excluding: 293368\n",
      "excluding: 293622\n",
      "excluding: 294099\n",
      "excluding: 294641\n",
      "excluding: 295338\n",
      "excluding: 295834\n",
      "excluding: 296104\n",
      "excluding: 297313\n",
      "excluding: 297801\n",
      "excluding: 297962\n",
      "excluding: 298516\n",
      "excluding: 298724\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../data/analysis.csv\"\n",
    "df = import_data(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d6b019c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318433, 97)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d9e9371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>icu_admissiontime</th>\n",
       "      <th>icu_dischargetime</th>\n",
       "      <th>timer</th>\n",
       "      <th>timer_dt</th>\n",
       "      <th>glc</th>\n",
       "      <th>glc_dt</th>\n",
       "      <th>msk</th>\n",
       "      <th>...</th>\n",
       "      <th>demo_gender_M</th>\n",
       "      <th>demo_ethnicity_grouped_asian</th>\n",
       "      <th>demo_ethnicity_grouped_black</th>\n",
       "      <th>demo_ethnicity_grouped_hispanic</th>\n",
       "      <th>demo_ethnicity_grouped_native</th>\n",
       "      <th>demo_ethnicity_grouped_other</th>\n",
       "      <th>demo_ethnicity_grouped_unknown</th>\n",
       "      <th>demo_ethnicity_grouped_white</th>\n",
       "      <th>t0</th>\n",
       "      <th>t1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48083</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>6.52</td>\n",
       "      <td>9.65</td>\n",
       "      <td>139.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.65</td>\n",
       "      <td>16.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48084</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>9.65</td>\n",
       "      <td>15.30</td>\n",
       "      <td>139.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.30</td>\n",
       "      <td>24.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48085</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>15.30</td>\n",
       "      <td>21.62</td>\n",
       "      <td>140.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.62</td>\n",
       "      <td>36.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48086</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>21.62</td>\n",
       "      <td>27.12</td>\n",
       "      <td>153.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.12</td>\n",
       "      <td>48.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48087</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>27.12</td>\n",
       "      <td>33.12</td>\n",
       "      <td>109.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.12</td>\n",
       "      <td>60.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48088</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>33.12</td>\n",
       "      <td>39.70</td>\n",
       "      <td>111.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.70</td>\n",
       "      <td>72.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48089</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>39.70</td>\n",
       "      <td>45.12</td>\n",
       "      <td>124.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45.12</td>\n",
       "      <td>84.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48090</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>45.12</td>\n",
       "      <td>52.12</td>\n",
       "      <td>127.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.12</td>\n",
       "      <td>97.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48091</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>52.12</td>\n",
       "      <td>57.42</td>\n",
       "      <td>117.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.42</td>\n",
       "      <td>109.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48092</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>57.42</td>\n",
       "      <td>63.12</td>\n",
       "      <td>120.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.12</td>\n",
       "      <td>120.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48093</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>63.12</td>\n",
       "      <td>87.77</td>\n",
       "      <td>110.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87.77</td>\n",
       "      <td>150.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48094</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>87.77</td>\n",
       "      <td>93.18</td>\n",
       "      <td>106.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.18</td>\n",
       "      <td>180.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48095</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>93.18</td>\n",
       "      <td>99.57</td>\n",
       "      <td>134.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99.57</td>\n",
       "      <td>192.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48096</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>99.57</td>\n",
       "      <td>104.50</td>\n",
       "      <td>134.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>104.50</td>\n",
       "      <td>204.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48097</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>104.50</td>\n",
       "      <td>110.60</td>\n",
       "      <td>134.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110.60</td>\n",
       "      <td>215.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48098</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>110.60</td>\n",
       "      <td>117.12</td>\n",
       "      <td>116.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>117.12</td>\n",
       "      <td>227.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48099</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>117.12</td>\n",
       "      <td>122.95</td>\n",
       "      <td>120.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122.95</td>\n",
       "      <td>240.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48100</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>122.95</td>\n",
       "      <td>129.12</td>\n",
       "      <td>139.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>129.12</td>\n",
       "      <td>252.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48101</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>129.12</td>\n",
       "      <td>134.98</td>\n",
       "      <td>122.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134.98</td>\n",
       "      <td>264.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48102</th>\n",
       "      <td>87977</td>\n",
       "      <td>100011</td>\n",
       "      <td>214619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.07</td>\n",
       "      <td>134.98</td>\n",
       "      <td>148.25</td>\n",
       "      <td>137.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148.25</td>\n",
       "      <td>283.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id  hadm_id  icustay_id  icu_admissiontime  icu_dischargetime  \\\n",
       "48083       87977   100011      214619                0.0             276.07   \n",
       "48084       87977   100011      214619                0.0             276.07   \n",
       "48085       87977   100011      214619                0.0             276.07   \n",
       "48086       87977   100011      214619                0.0             276.07   \n",
       "48087       87977   100011      214619                0.0             276.07   \n",
       "48088       87977   100011      214619                0.0             276.07   \n",
       "48089       87977   100011      214619                0.0             276.07   \n",
       "48090       87977   100011      214619                0.0             276.07   \n",
       "48091       87977   100011      214619                0.0             276.07   \n",
       "48092       87977   100011      214619                0.0             276.07   \n",
       "48093       87977   100011      214619                0.0             276.07   \n",
       "48094       87977   100011      214619                0.0             276.07   \n",
       "48095       87977   100011      214619                0.0             276.07   \n",
       "48096       87977   100011      214619                0.0             276.07   \n",
       "48097       87977   100011      214619                0.0             276.07   \n",
       "48098       87977   100011      214619                0.0             276.07   \n",
       "48099       87977   100011      214619                0.0             276.07   \n",
       "48100       87977   100011      214619                0.0             276.07   \n",
       "48101       87977   100011      214619                0.0             276.07   \n",
       "48102       87977   100011      214619                0.0             276.07   \n",
       "\n",
       "        timer  timer_dt    glc  glc_dt  msk  ...  demo_gender_M  \\\n",
       "48083    6.52      9.65  139.0     NaN  1.0  ...              1   \n",
       "48084    9.65     15.30  139.0   140.0  0.0  ...              1   \n",
       "48085   15.30     21.62  140.0   153.0  0.0  ...              1   \n",
       "48086   21.62     27.12  153.0   109.0  0.0  ...              1   \n",
       "48087   27.12     33.12  109.0   111.0  0.0  ...              1   \n",
       "48088   33.12     39.70  111.0   124.0  0.0  ...              1   \n",
       "48089   39.70     45.12  124.0   127.0  0.0  ...              1   \n",
       "48090   45.12     52.12  127.0   117.0  0.0  ...              1   \n",
       "48091   52.12     57.42  117.0   120.0  0.0  ...              1   \n",
       "48092   57.42     63.12  120.0   110.0  0.0  ...              1   \n",
       "48093   63.12     87.77  110.0   106.0  0.0  ...              1   \n",
       "48094   87.77     93.18  106.0   134.0  0.0  ...              1   \n",
       "48095   93.18     99.57  134.0     NaN  1.0  ...              1   \n",
       "48096   99.57    104.50  134.0     NaN  1.0  ...              1   \n",
       "48097  104.50    110.60  134.0   116.0  0.0  ...              1   \n",
       "48098  110.60    117.12  116.0   120.0  0.0  ...              1   \n",
       "48099  117.12    122.95  120.0   139.0  0.0  ...              1   \n",
       "48100  122.95    129.12  139.0   122.0  0.0  ...              1   \n",
       "48101  129.12    134.98  122.0   137.0  0.0  ...              1   \n",
       "48102  134.98    148.25  137.0   145.0  0.0  ...              1   \n",
       "\n",
       "       demo_ethnicity_grouped_asian  demo_ethnicity_grouped_black  \\\n",
       "48083                             0                             0   \n",
       "48084                             0                             0   \n",
       "48085                             0                             0   \n",
       "48086                             0                             0   \n",
       "48087                             0                             0   \n",
       "48088                             0                             0   \n",
       "48089                             0                             0   \n",
       "48090                             0                             0   \n",
       "48091                             0                             0   \n",
       "48092                             0                             0   \n",
       "48093                             0                             0   \n",
       "48094                             0                             0   \n",
       "48095                             0                             0   \n",
       "48096                             0                             0   \n",
       "48097                             0                             0   \n",
       "48098                             0                             0   \n",
       "48099                             0                             0   \n",
       "48100                             0                             0   \n",
       "48101                             0                             0   \n",
       "48102                             0                             0   \n",
       "\n",
       "       demo_ethnicity_grouped_hispanic  demo_ethnicity_grouped_native  \\\n",
       "48083                                1                              0   \n",
       "48084                                1                              0   \n",
       "48085                                1                              0   \n",
       "48086                                1                              0   \n",
       "48087                                1                              0   \n",
       "48088                                1                              0   \n",
       "48089                                1                              0   \n",
       "48090                                1                              0   \n",
       "48091                                1                              0   \n",
       "48092                                1                              0   \n",
       "48093                                1                              0   \n",
       "48094                                1                              0   \n",
       "48095                                1                              0   \n",
       "48096                                1                              0   \n",
       "48097                                1                              0   \n",
       "48098                                1                              0   \n",
       "48099                                1                              0   \n",
       "48100                                1                              0   \n",
       "48101                                1                              0   \n",
       "48102                                1                              0   \n",
       "\n",
       "       demo_ethnicity_grouped_other  demo_ethnicity_grouped_unknown  \\\n",
       "48083                             0                               0   \n",
       "48084                             0                               0   \n",
       "48085                             0                               0   \n",
       "48086                             0                               0   \n",
       "48087                             0                               0   \n",
       "48088                             0                               0   \n",
       "48089                             0                               0   \n",
       "48090                             0                               0   \n",
       "48091                             0                               0   \n",
       "48092                             0                               0   \n",
       "48093                             0                               0   \n",
       "48094                             0                               0   \n",
       "48095                             0                               0   \n",
       "48096                             0                               0   \n",
       "48097                             0                               0   \n",
       "48098                             0                               0   \n",
       "48099                             0                               0   \n",
       "48100                             0                               0   \n",
       "48101                             0                               0   \n",
       "48102                             0                               0   \n",
       "\n",
       "       demo_ethnicity_grouped_white      t0      t1  \n",
       "48083                             0    9.65   16.17  \n",
       "48084                             0   15.30   24.95  \n",
       "48085                             0   21.62   36.92  \n",
       "48086                             0   27.12   48.74  \n",
       "48087                             0   33.12   60.24  \n",
       "48088                             0   39.70   72.82  \n",
       "48089                             0   45.12   84.82  \n",
       "48090                             0   52.12   97.24  \n",
       "48091                             0   57.42  109.54  \n",
       "48092                             0   63.12  120.54  \n",
       "48093                             0   87.77  150.89  \n",
       "48094                             0   93.18  180.95  \n",
       "48095                             0   99.57  192.75  \n",
       "48096                             0  104.50  204.07  \n",
       "48097                             0  110.60  215.10  \n",
       "48098                             0  117.12  227.72  \n",
       "48099                             0  122.95  240.07  \n",
       "48100                             0  129.12  252.07  \n",
       "48101                             0  134.98  264.10  \n",
       "48102                             0  148.25  283.23  \n",
       "\n",
       "[20 rows x 99 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.icustay_id == 214619].iloc[0:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b351caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['glc'] = df.groupby('icustay_id').glc.fillna(method='ffill')\n",
    "df = df.loc[~df.glc.isnull()]\n",
    "\n",
    "for id_ in df.icustay_id.unique():\n",
    "    df_id = df.loc[df.icustay_id == id_,:]\n",
    "    if (sum(df_id.msk) == df_id.shape[0]):\n",
    "        df.drop(df.loc[df.icustay_id == id_,:].index,inplace=True)\n",
    "        print(\"excluding:\",id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baba61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.feature_sets import all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65807b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glc', 'input_short_injection', 'input_short_push', 'input_intermediate', 'input_long', 'input_hrs', 'demo_admission_age', 'demo_weight_first', 'demo_height_first', 'demo_diabetes_uncomplicated', 'demo_diabetes_complicated', 'elix_congestive_heart_failure', 'elix_cardiac_arrhythmias', 'elix_valvular_disease', 'elix_pulmonary_circulation', 'elix_peripheral_vascular', 'elix_hypertension', 'elix_paralysis', 'elix_other_neurological', 'elix_chronic_pulmonary', 'elix_hypothyroidism', 'elix_renal_failure', 'elix_liver_disease', 'elix_peptic_ulcer', 'elix_aids', 'elix_lymphoma', 'elix_metastatic_cancer', 'elix_solid_tumor', 'elix_rheumatoid_arthritis', 'elix_coagulopathy', 'elix_obesity', 'elix_weight_loss', 'elix_fluid_electrolyte', 'elix_blood_loss_anemia', 'elix_deficiency_anemias', 'elix_alcohol_abuse', 'elix_drug_abuse', 'elix_psychoses', 'elix_depression', 'vital_heartrate', 'vital_sysbp', 'vital_dbp', 'vital_meanbp', 'vital_resprate', 'vital_tmpc', 'vital_spo2', 'ventilated', 'med_dopamine_rate', 'med_dobutamine_rate', 'med_milirinone_rate', 'med_phenylephrine_rate', 'med_epinephrine_rate', 'med_norepinephrine_rate', 'fluid_rate_all', 'fluid_rate_in', 'fluid_rate_out', 'lab_aniongap', 'lab_albumin', 'lab_bands', 'lab_bicarbonate', 'lab_bilirubin', 'lab_creatinine', 'lab_chloride', 'lab_hematocrit', 'lab_hemoglobin', 'lab_lactate', 'lab_platelet', 'lab_potassium', 'lab_ptt', 'lab_inr', 'lab_pt', 'lab_sodium', 'lab_bun', 'lab_wbc', 'dialysis_present', 'dialysis_active', 'demo_gender_F', 'demo_gender_M', 'demo_ethnicity_grouped_asian', 'demo_ethnicity_grouped_black', 'demo_ethnicity_grouped_hispanic', 'demo_ethnicity_grouped_native', 'demo_ethnicity_grouped_other', 'demo_ethnicity_grouped_unknown', 'demo_ethnicity_grouped_white']\n"
     ]
    }
   ],
   "source": [
    "print(all_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d05dee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['input'] = df['input']/50\n",
    "# df['input_hrs'] = df['input_hrs']/30\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from src.data.data_scaler import PreProcess\n",
    "\n",
    "FEATURES = all_features()\n",
    "NFEATURES = len(all_features())\n",
    "preproc = PreProcess(FEATURES,QuantileTransformer())\n",
    "\n",
    "train_ids, test_ids = train_test_split(df.icustay_id.unique(),test_size=0.25)\n",
    "\n",
    "df_train = df.loc[df.icustay_id.isin(train_ids)]\n",
    "df_valid = df.loc[df.icustay_id.isin(test_ids)]\n",
    "preproc.fit(df_train)\n",
    "df_train = preproc.transform(df_train)\n",
    "df_valid = preproc.transform(df_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "febe2891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_train['input_short_injection'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d30cbadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_valid['input_short_injection'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "273a8a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_VARS = ['glc', 'input_short_injection', 'input_short_push', 'input_intermediate', 'input_long', 'input_hrs']\n",
    "NFEATURES = len(FEATURE_VARS)\n",
    "TIME_VARS = [\"timer\",\"timer_dt\"]\n",
    "\n",
    "class MIMICDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        patientunitstayids: \n",
    "        df:\n",
    "        ...\n",
    "    \n",
    "    Example:\n",
    "    \"\"\"\n",
    "    def __init__(self,df,verbose=True):\n",
    "        self.X,self.y,self.msk,self.dt = self.load_data(df,verbose=verbose)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx],self.y[idx],self.msk[idx],self.dt[idx]\n",
    "    \n",
    "    def load_data(self,df,verbose):\n",
    "        X_list, y_list, msk_list, dt_list = [], [], [], []\n",
    "        ids = df.icustay_id.unique()\n",
    "        if verbose:\n",
    "            print(\"reconfiguring data...\")\n",
    "        for id_ in ids:\n",
    "            df_id = df.loc[df.icustay_id == id_,:]\n",
    "            if (sum(df_id.msk) == df_id.shape[0]):\n",
    "                print(\"excluding:\",id_)\n",
    "                continue\n",
    "            X = df_id.loc[:,FEATURE_VARS]\n",
    "            y = df_id.loc[:,\"glc_dt\"]\n",
    "            msk = df_id.loc[:,\"msk\"]\n",
    "            dt = df_id.loc[:,TIME_VARS]\n",
    "            X = np.array(X).astype(np.float32)\n",
    "            y = np.array(y).astype(np.float32)\n",
    "            msk = np.array(msk).astype(np.int32)\n",
    "            dt = np.array(dt).astype(np.float32)\n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "            msk_list.append(msk)\n",
    "            dt_list.append(dt)\n",
    "        return X_list,y_list,msk_list,dt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf1942e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconfiguring data...\n",
      "reconfiguring data...\n"
     ]
    }
   ],
   "source": [
    "dl_train = DataLoader(MIMICDataset(df_train),batch_size=1)\n",
    "dl_valid = DataLoader(MIMICDataset(df_valid),batch_size=1)\n",
    "dataloaders = {'train':dl_train,'validation':dl_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6ba4e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from torchdiffeq import odeint_adjoint as odeint\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "import math\n",
    "\n",
    "# Base ----------------------------------------------------------------------------\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, p, output_dim, device):\n",
    "        super(Baseline, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.p = p\n",
    "        self.device = device\n",
    "        \n",
    "    def train_single_epoch(self,dataloader,optim):\n",
    "        loss = 0.0\n",
    "        for i, (x, y, msk, dt) in enumerate(dataloader):\n",
    "            x = x.to(self.device)\n",
    "            y = y.view(-1,1).to(self.device)\n",
    "            dt = dt.to(self.device)\n",
    "            msk = msk.bool().to(self.device)\n",
    "            optim.zero_grad()\n",
    "            preds = self.forward(dt,x)\n",
    "            loss_step = self.loss_fn(preds,y,~msk.squeeze(0))\n",
    "            loss_step.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(self.parameters(), 10.0)\n",
    "            optim.step()\n",
    "            loss += loss_step.item()\n",
    "            if i % 1e3 == 0:\n",
    "                print(\"BATCH_loss : {:05.3f}\".format(loss_step.item()))\n",
    "        loss /= (i + 1)\n",
    "        print(\"EPOCH_loss : {:05.3f}\".format(loss))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def evaluate(self,dataloader,p=0.0):\n",
    "        rmse, loss = 0., 0.\n",
    "        N = 0\n",
    "        y_preds = []\n",
    "        y_tests = []\n",
    "        msks = []\n",
    "        #dts = []\n",
    "        with tqdm(total=len(dataloader)) as t:\n",
    "            for i, (x, y, msk, dt) in enumerate(dataloader):\n",
    "                N += sum((msk == 0).squeeze(0)).item()\n",
    "                x = x.to(self.device)\n",
    "                y = y.view(-1,1).to(self.device)\n",
    "                dt = dt.to(self.device)\n",
    "                # model prediction\n",
    "                y_ = self.forward(dt,x)\n",
    "                y_preds.append([yc.detach().cpu().numpy() for yc in y_]) \n",
    "                y_tests.append(y.cpu().numpy())\n",
    "                msk = msk.bool().to(self.device)\n",
    "                rmse += self.get_sse(y_,y,~msk.squeeze(0)).item()\n",
    "                loss += self.loss_fn(y_,y,~msk.squeeze(0)).item()\n",
    "                msks.append(msk.cpu().numpy())\n",
    "                t.update()\n",
    "        rmse /= N\n",
    "        loss /= (i + 1)\n",
    "        rmse = math.sqrt(rmse)\n",
    "        print(\"_rmse : {:05.3f}\".format(rmse))\n",
    "        print(\"_loss : {:05.3f}\".format(loss))\n",
    "        return loss,rmse, y_preds, y_tests, msks\n",
    "\n",
    "    def get_sse(self,y_,y,msk):\n",
    "        \"\"\"\n",
    "        SSE: sum of squared errors\n",
    "        \"\"\"\n",
    "        if type(y_) == tuple:\n",
    "            y_ = y_[0]\n",
    "        c = torch.log(torch.tensor(140.0))\n",
    "        rmse = torch.sum((torch.exp(y_[msk] + c) - torch.exp(y[msk] + c))**2)\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef76909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e123c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# class ODEFunc(nn.Module):\n",
    "#     \"\"\"\n",
    "#     dglucose/dt = NN(glucose,insulin)\n",
    "#     \"\"\"\n",
    "#     def __init__(self,input_dim,hidden_dim):\n",
    "#         super(ODEFunc, self).__init__()\n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, 50),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(50, hidden_dim),\n",
    "#         )\n",
    "\n",
    "#         for m in self.net.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "#                 nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         return self.net(z)\n",
    "\n",
    "# class LatentODE1(Baseline):\n",
    "\n",
    "#     def __init__(self, input_dim, hidden_dim, p, output_dim, device):\n",
    "#         Baseline.__init__(self,input_dim, hidden_dim, p, output_dim, device)\n",
    "#         self.device = device\n",
    "#         self.encoder = nn.LSTMCell(input_dim,hidden_dim)\n",
    "#         self.func = ODEFunc(input_dim,hidden_dim).to(device)\n",
    "#         self.sigma = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, 1),\n",
    "#             nn.Softplus()\n",
    "#         )\n",
    "#         self.mu = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, dt, x):\n",
    "        \n",
    "        \n",
    "#         x = x.squeeze(0)\n",
    "#         dt = dt.squeeze(0)\n",
    "#         T = x.size(0)\n",
    "        \n",
    "#         mu_out,sigma_out = torch.zeros(T,1,device = self.device),torch.zeros(T,1,device = self.device) \n",
    "#         #z_t = torch.zeros(1,self.hidden_dim,device = self.device)\n",
    "#         z_1 = torch.zeros(1,self.hidden_dim,device = self.device)\n",
    "#         c_1 = torch.zeros(1,self.hidden_dim,device = self.device)\n",
    "#         for i in range(0,T):\n",
    "#             # encode \n",
    "#             z_1,c_1 = self.encoder(x[i].unsqueeze(0),(z_1.detach().clone(),c_1.detach().clone()))\n",
    "            \n",
    "#             # ODE\n",
    "#             z_2 = self.euler(self.func,z_1,dt[i],1.0)\n",
    "            \n",
    "#             # decode \n",
    "#             sigma = self.sigma(z_2)\n",
    "#             mu = self.mu(z_2)\n",
    "#             sigma_out[i] = sigma\n",
    "#             mu_out[i] = mu\n",
    "        \n",
    "#         return (mu_out,sigma_out)\n",
    "    \n",
    "#     def loss_fn(self,mu_s_,y,msk):\n",
    "#         y_, s_ = mu_s_\n",
    "#         distribution = torch.distributions.normal.Normal(y_[msk], s_[msk])\n",
    "#         likelihood = distribution.log_prob(y[msk])\n",
    "#         return -torch.mean(likelihood)\n",
    "    \n",
    "#     def euler(self,func,z0,t,h):\n",
    "#         if (t[1]-t[0]) == 0:\n",
    "#             return z0\n",
    "#         else:\n",
    "#             tsteps = torch.linspace(t[0],t[1],int(2+(t[1]-t[0]) // h))\n",
    "#             hs = torch.diff(tsteps)\n",
    "#             #y = copy.deepcopy(y0)\n",
    "#             z = z0.detach().clone()\n",
    "#             for h in hs:\n",
    "#                 z += h*func(z.detach().clone())\n",
    "#         return z \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3470e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# class ODEFunc(nn.Module):\n",
    "#     \"\"\"\n",
    "#     dglucose/dt = NN(glucose,insulin)\n",
    "#     \"\"\"\n",
    "#     def __init__(self,input_dim,hidden_dim):\n",
    "#         super(ODEFunc, self).__init__()\n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(input_dim+hidden_dim, 50),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(50, hidden_dim),\n",
    "#         )\n",
    "\n",
    "#         for m in self.net.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "#                 nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "#     def forward(self, z, x=None):\n",
    "#         if x != None:\n",
    "#             zx = torch.cat((z,x),1)\n",
    "#         else:\n",
    "#             zx = z\n",
    "#         return self.net(zx)\n",
    "    \n",
    "# class LatentODE2(Baseline):\n",
    "\n",
    "#     def __init__(self, input_dim, hidden_dim, p, output_dim, device):\n",
    "#         Baseline.__init__(self,input_dim, hidden_dim, p, output_dim, device)\n",
    "#         self.device = device\n",
    "#         self.func = ODEFunc(input_dim,hidden_dim).to(device)\n",
    "#         self.dfunc = ODEFunc(0,hidden_dim).to(device)\n",
    "#         self.sigma = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(hidden_dim, 1),\n",
    "#             nn.Softplus()\n",
    "#         )\n",
    "#         self.mu = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(hidden_dim, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, dt, x):\n",
    "        \n",
    "        \n",
    "#         x = x.squeeze(0)\n",
    "#         dt = dt.squeeze(0)\n",
    "#         T = x.size(0)\n",
    "        \n",
    "#         mu_out,sigma_out= torch.zeros(T,1,device = self.device),torch.zeros(T,1,device = self.device) \n",
    "#         z_t = torch.zeros(1,self.hidden_dim,device = self.device)\n",
    "#         for i in range(0,T):\n",
    "#             # update hidden state\n",
    "#             z_t = self.euler1(self.dfunc,z_t.clone(),dt[i],h=1.0)\n",
    "            \n",
    "#             # encode ODE\n",
    "#             x_i = x[i].unsqueeze(0)\n",
    "#             z_t = self.euler(self.func,z_t.clone(),x_i,dt[i],h=1.0)\n",
    "            \n",
    "#             # decode \n",
    "#             sigma = self.sigma(z_t.clone())\n",
    "#             mu = self.mu(z_t.clone())\n",
    "#             sigma_out[i] = sigma\n",
    "#             mu_out[i] = mu\n",
    "        \n",
    "#         return (mu_out,sigma_out)\n",
    "    \n",
    "#     def loss_fn(self,mu_s_,y,msk):\n",
    "#         y_, s_ = mu_s_\n",
    "#         distribution = torch.distributions.normal.Normal(y_[msk], s_[msk])\n",
    "#         likelihood = distribution.log_prob(y[msk])\n",
    "#         return -torch.mean(likelihood)\n",
    "\n",
    "#     def euler(self,func,y0,x,t,h):\n",
    "#         if (t[1]-t[0]) == 0:\n",
    "#             return y0\n",
    "#         else :\n",
    "#             tsteps = torch.linspace(t[0],t[1],int(2+(t[1]-t[0]) // h))\n",
    "#             hs = torch.diff(tsteps)\n",
    "#             #y = copy.deepcopy(y0)\n",
    "#             y = y0\n",
    "#             for h in hs:\n",
    "#                 y += h*func(y,x)\n",
    "#         return y \n",
    "\n",
    "#     def euler1(self,func,y0,t,h):\n",
    "#         if (t[1]-t[0]) == 0:\n",
    "#             return y0\n",
    "#         else :\n",
    "#             tsteps = torch.linspace(t[0],t[1],int(2+(t[1]-t[0]) // h))\n",
    "#             hs = torch.diff(tsteps)\n",
    "#             #y = copy.deepcopy(y0)\n",
    "#             y = y0.detach().clone()\n",
    "#             for h in hs:\n",
    "#                 y += h*func(y.detach().clone())\n",
    "#         return y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0e5e8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"\n",
    "    dglucose/dt = NN(glucose,insulin)\n",
    "    \"\"\"\n",
    "    def __init__(self,input_dim,hidden_dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim+hidden_dim, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, hidden_dim),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        zx = torch.cat((z,x),1)\n",
    "        return self.net(zx)\n",
    "    \n",
    "class LatentODE(Baseline):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, p, output_dim, device):\n",
    "        Baseline.__init__(self,input_dim, hidden_dim, p, output_dim, device)\n",
    "        self.device = device\n",
    "        self.func = ODEFunc(input_dim,hidden_dim).to(device)\n",
    "        self.sigma = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        self.mu = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, dt, x):\n",
    "        \n",
    "        \n",
    "        x = x.squeeze(0)\n",
    "        dt = dt.squeeze(0)\n",
    "        T = x.size(0)\n",
    "        \n",
    "        mu_out,sigma_out= torch.zeros(T,1,device = self.device),torch.zeros(T,1,device = self.device) \n",
    "        z_t = torch.zeros(1,self.hidden_dim,device = self.device)\n",
    "        for i in range(0,T):\n",
    "            \n",
    "            # embedding layer\n",
    "            # ....\n",
    "            \n",
    "            # encode - ODE\n",
    "            x_i = x[i].unsqueeze(0)\n",
    "            z_t = self.euler(self.func,z_t.clone(),x_i,dt[i],h=1.0)\n",
    "            \n",
    "            # output layer \n",
    "            sigma = self.sigma(z_t.clone())\n",
    "            mu = self.mu(z_t.clone())\n",
    "            sigma_out[i] = sigma\n",
    "            mu_out[i] = mu\n",
    "        \n",
    "        return (mu_out,sigma_out)\n",
    "    \n",
    "    def loss_fn(self,mu_s_,y,msk):\n",
    "        y_, s_ = mu_s_\n",
    "        distribution = torch.distributions.normal.Normal(y_[msk], s_[msk])\n",
    "        likelihood = distribution.log_prob(y[msk])\n",
    "        return -torch.mean(likelihood)\n",
    "\n",
    "    def euler(self,func,y0,x,t,h):\n",
    "        if (t[1]-t[0]) == 0:\n",
    "            return y0\n",
    "        else :\n",
    "            tsteps = torch.linspace(t[0],t[1],int(2+(t[1]-t[0]) // h))\n",
    "            hs = torch.diff(tsteps)\n",
    "            #y = copy.deepcopy(y0)\n",
    "            y = y0\n",
    "            for h in hs:\n",
    "                y += h*func(y,x)\n",
    "        return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ab686ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LODEFunc(nn.Module):\n",
    "    \"\"\"\n",
    "    dglucose/dt = NN(glucose,insulin)\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_dim,input_dim,device):\n",
    "        super(LODEFunc, self).__init__()\n",
    "\n",
    "        self.x = torch.zeros(128,100,input_dim).to(device)\n",
    "        self.dt = torch.zeros(128,100,1).to(device)\n",
    "        self.device = device\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim+hidden_dim, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, hidden_dim),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        xz = torch.cat((z,self.x),1)\n",
    "        out = self.net(xz)\n",
    "        out = out * (self.dt/24) # -> scale by timestep\n",
    "        return out \n",
    "\n",
    "    def solve_ode(self, z0, t, x):\n",
    "        self.x = x  # overwrites\n",
    "        self.dt = t\n",
    "#         t24 = t.item()/24\n",
    "#         outputs = odeint(self, z0, torch.tensor([0,t24]).to(self.device),method=\"euler\",options=dict(step_size=max(0.1,t24/4)))[1]\n",
    "        #t24 = t.item()/24\n",
    "        outputs = odeint(self, z0, torch.tensor([0.,1.]).to(self.device),method=\"euler\",options=dict(step_size=0.25))[1]\n",
    "        return outputs\n",
    "    \n",
    "    def euler(self,func,y0,t,h):\n",
    "        if (t[1]-t[0]) == 0:\n",
    "            return y0\n",
    "        else :\n",
    "            \n",
    "            tsteps = torch.linspace(t[0],t[1],int(2+(t[1]-t[0]) // h))\n",
    "            hs = torch.diff(tsteps)\n",
    "            #y = copy.deepcopy(y0)\n",
    "            y = y0\n",
    "            for h in hs:\n",
    "                y += h*self(0,y)\n",
    "        return y \n",
    "\n",
    "class latentODE(Baseline):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, p, output_dim, device):\n",
    "        Baseline.__init__(self,input_dim, hidden_dim, p, output_dim, device)\n",
    "        self.device = device\n",
    "        self.func = LODEFunc(hidden_dim,input_dim,device).to(device)\n",
    "        self.mu_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        ).to(device)\n",
    "        self.sigma_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Softplus()\n",
    "        ).to(device)\n",
    "        self.encode_z0 = nn.Sequential(\n",
    "            nn.Linear(input_dim, max(hidden_dim,hidden_dim) // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(max(hidden_dim,hidden_dim) // 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        ).to(device)\n",
    "        \n",
    "#         for m in self.encode_z0.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "#                 nn.init.constant_(m.bias, val=0)\n",
    "        \n",
    "    def forward(self, dt, x):\n",
    "        \n",
    "        x = x.squeeze(0)\n",
    "        dt = dt.squeeze(0)\n",
    "        T = x.shape[0]\n",
    "                \n",
    "        mu_out,s_out= torch.zeros(T,1,device = self.device),torch.zeros(T,1,device = self.device) \n",
    "        z_t = torch.zeros(1,self.hidden_dim,device = self.device)\n",
    "        \n",
    "        for i in range(0,T):\n",
    "            x_i = x[i:(i+1),:]\n",
    "            dt_i = (dt[i][1] - dt[i][0])\n",
    "            if dt_i.item() > 0:\n",
    "                z_t = self.func.solve_ode(z_t.clone(),dt_i,x_i)\n",
    "            else:\n",
    "                z_t = z_t\n",
    "            mu_out[i] = self.mu_net(z_t)\n",
    "            s_out[i] = self.sigma_net(z_t)\n",
    "\n",
    "        return (mu_out,s_out)\n",
    "    \n",
    "    def loss_fn(self,mu_s_,y,msk):\n",
    "        y_, s_ = mu_s_\n",
    "        distribution = torch.distributions.normal.Normal(y_[msk], s_[msk])\n",
    "        likelihood = distribution.log_prob(y[msk])\n",
    "        return -torch.mean(likelihood)\n",
    "\n",
    "    \n",
    "#     def loss_fn(self,y_,y,msk):\n",
    "#         return sum((y_[msk] - y[msk])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e9c683e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = latentODE(NFEATURES, 8, 0.5, 1, \"cpu\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ba744339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NFEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d22be3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.1324],\n",
      "        [-0.1268],\n",
      "        [-0.1266],\n",
      "        [-0.1237],\n",
      "        [-0.1230],\n",
      "        [-0.1181],\n",
      "        [-0.1154]], grad_fn=<CopySlices>), tensor([[0.7812],\n",
      "        [0.7797],\n",
      "        [0.7790],\n",
      "        [0.7778],\n",
      "        [0.7771],\n",
      "        [0.7747],\n",
      "        [0.7731]], grad_fn=<CopySlices>))\n",
      "tensor([[0.7812],\n",
      "        [0.7797],\n",
      "        [0.7790],\n",
      "        [0.7778],\n",
      "        [0.7771],\n",
      "        [0.7747],\n",
      "        [0.7731]], grad_fn=<CopySlices>)\n",
      "torch.Size([7, 1])\n",
      "torch.Size([7, 1])\n"
     ]
    }
   ],
   "source": [
    "t = next(iter(dl_train))[3].squeeze(0).to(\"cpu\")\n",
    "x = next(iter(dl_train))[0].squeeze(0).to(\"cpu\")\n",
    "y = next(iter(dl_train))[1].squeeze(0)\n",
    "msk = next(iter(dl_train))[2].squeeze(0).to(\"cpu\")\n",
    "y_ = model(t,x)\n",
    "model.loss_fn(y_,y.to(\"cpu\"),~msk.bool())\n",
    "print(y_)\n",
    "print(y_[1])\n",
    "print(y_[0].shape)\n",
    "print(y_[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "20c9dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optim = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "# BATCH_loss : 0.700\n",
    "# BATCH_loss : -0.327\n",
    "# BATCH_loss : -0.240\n",
    "# BATCH_loss : -0.239\n",
    "# BATCH_loss : -0.118\n",
    "# BATCH_loss : 0.011\n",
    "# BATCH_loss : 0.465\n",
    "# BATCH_loss : -0.378\n",
    "# BATCH_loss : -0.289\n",
    "# BATCH_loss : -0.422\n",
    "\n",
    "# BATCH_loss : 0.674\n",
    "# BATCH_loss : -0.411\n",
    "# BATCH_loss : -0.156\n",
    "# BATCH_loss : 0.130\n",
    "# BATCH_loss : -0.041\n",
    "# BATCH_loss : 0.076\n",
    "# BATCH_loss : 1.250\n",
    "# BATCH_loss : -0.286\n",
    "# BATCH_loss : -0.134\n",
    "# BATCH_loss : -0.398\n",
    "# BATCH_loss : 0.553\n",
    "# BATCH_loss : -0.048\n",
    "# BATCH_loss : 0.284\n",
    "# EPOCH_loss : 0.149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "acf1b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_loss : 0.975\n",
      "BATCH_loss : -0.366\n",
      "BATCH_loss : -0.100\n",
      "BATCH_loss : -0.029\n",
      "BATCH_loss : -0.045\n",
      "BATCH_loss : 0.008\n",
      "BATCH_loss : 1.353\n",
      "BATCH_loss : -0.310\n",
      "BATCH_loss : -0.182\n",
      "BATCH_loss : -0.371\n",
      "BATCH_loss : 0.523\n",
      "BATCH_loss : -0.079\n",
      "BATCH_loss : 0.159\n",
      "EPOCH_loss : 0.127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12707904078092686"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_single_epoch(dl_train,model_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a737e6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4123/4123 [01:08<00:00, 60.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_rmse : 43.624\n",
      "_loss : 0.068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss,rmse, y_preds, y_tests, msks = model.evaluate(dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2f1519bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.27153708799015"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse\n",
    "# _rmse : 48.872\n",
    "# _loss : 0.123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd81ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_valid.glc,df_valid.glc_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71593293",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid[\"glc_dt_locf\"] = df_valid.glc\n",
    "df_valid.loc[df_valid.glc_dt_locf == 0,'glc_dt_locf'] = np.NaN\n",
    "df_valid[\"glc_dt_locf\"] = df_valid.groupby('icustay_id')[\"glc_dt_locf\"].ffill()\n",
    "res_rmse_locf = math.sqrt(np.mean((ginv(df_valid.glc_dt_locf[df_valid.msk==0]) - ginv(df_valid.glc_dt[df_valid.msk==0]))**2))\n",
    "print(\"RMSE (locf): {:05.4f}\".format(res_rmse_locf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "math.sqrt(np.mean((df_valid.glc[df_valid.msk==0] - df_valid.glc_dt[df_valid.msk==0])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f538a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse / res_rmse_locf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_preds = [y[0] for y in y_preds]\n",
    "s_preds = [y[1] for y in y_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4add60",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_preds = np.concatenate(mu_preds)\n",
    "y_tests = np.concatenate(y_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b6bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_preds = np.concatenate(s_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c87094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['mu_preds'] = mu_preds\n",
    "df_valid['s_preds'] = s_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fab60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = np.linspace(0,11,12)\n",
    "dts = np.append(dts,12.)\n",
    "for i in range(dts.shape[0]-1):\n",
    "    print('time ahead (hours): {:05.2f}-{:05.2f}'.format(dts[i],dts[i+1]))\n",
    "    msk = (df_valid.msk==0) & (df_valid.dt >= dts[i]) & (df_valid.dt < dts[i+1])\n",
    "    print(\"N: {:5}\".format(sum(msk)))\n",
    "    res_rmse_locf = math.sqrt(np.mean((ginv(df_valid.glc_dt_locf[msk]) - ginv(df_valid.glc_dt[msk]))**2))\n",
    "    print(\"RMSE (locf): {:05.4f}\".format(res_rmse_locf))\n",
    "    res_rmse_model = math.sqrt(np.mean((ginv(df_valid.mu_preds[msk]) - ginv(df_valid.glc_dt[msk]))**2))\n",
    "    print(\"RMSE (model): {:05.4f}\".format(res_rmse_model))\n",
    "    print(\"{:05.4f}\".format(res_rmse_model/res_rmse_locf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f412baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df_valid.icustay_id.unique()\n",
    "id_ = random.sample(list(ids),1)\n",
    "print(id_)\n",
    "df_i = df_valid.loc[(df_valid.icustay_id == id_[0])]\n",
    "plt.scatter(df_i.t1,df_i.glc_dt)\n",
    "plt.plot(df_i.t1[df_i.msk == 0],df_i.mu_preds[df_i.msk == 0],'-r')\n",
    "plt.plot(df_i.t1[df_i.msk == 0],df_i.mu_preds[df_i.msk == 0] + 2*df_i.s_preds[df_i.msk == 0],'--r')\n",
    "plt.plot(df_i.t1[df_i.msk == 0],df_i.mu_preds[df_i.msk == 0] - 2*df_i.s_preds[df_i.msk == 0],'--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b993bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "msks = np.concatenate([msk[0] for msk in msks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08099401",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_test = scipy.stats.norm(mu_preds[~msks].reshape(-1,1), s_preds[~msks].reshape(-1,1)).cdf(y_tests[~msks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prob_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb49cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"variance of probabilities:\")\n",
    "np.var(prob_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90463f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mu_preds[msks],y_tests[msks])\n",
    "xs = np.linspace(-1.5,1.5)\n",
    "plt.plot(xs,xs,'--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae421707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbdaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18917f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad9ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a8761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e82449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d6b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e66b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = next(iter(dl_train))[3].squeeze(0).to(\"cuda\")\n",
    "x = next(iter(dl_train))[0].squeeze(0).to(\"cuda\")\n",
    "y = next(iter(dl_train))[1].squeeze(0)\n",
    "msk = next(iter(dl_train))[2].squeeze(0)\n",
    "y_ = model(t,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE_VARS = [\"injection\",\"input_hrs\",\"input\",\"glc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b069b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.tensor([[0.0000,  0.0000, 0.000, 0.3412],\n",
    "        [ 0.0000,  0.0000,  0.0000, 0.2412]], device='cuda:0')\n",
    "print(ginv(xx[:,3].cpu().numpy()))\n",
    "tt = torch.tensor([[0.0081, 1.0081],\n",
    "        [3.0081, 3.4081]], device='cuda:0')\n",
    "ys = model(tt,xx)\n",
    "ginv(ys[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4580c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = model(tt,xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae097edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ginv(ys[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b844d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((df_train.injection > 0) & (df_train.input > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283bafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(s_preds[s_preds<1.0],bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bbd8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3825d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut out dimension 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentODE1_(nn.Module):\n",
    "    \"\"\"\n",
    "    dglucose/dt = NN(glucose,insulin)\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_dim,input_dim,batch_size,device):\n",
    "        super(LatentODE1_, self).__init__()\n",
    "        \n",
    "        self.x = torch.zeros(batch_size,SEQUENCE_LENGTH,input_dim).to(device)\n",
    "        self.dt = torch.zeros(batch_size,SEQUENCE_LENGTH,1).to(device)\n",
    "        self.device = device\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim+hidden_dim, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        xz = torch.cat((z,self.x),1)\n",
    "        return self.net(xz)*(self.dt*DT_SCALER) # -> scale by timestep\n",
    "    \n",
    "    def solve_ode(self, z0, t, x):\n",
    "        self.x = x  # overwrites\n",
    "        self.dt = t\n",
    "        outputs = odeint(self, z0, torch.tensor([0,1.0]).to(self.device),method='euler',options=dict(step_size=0.25))[1]\n",
    "        return outputs\n",
    "    \n",
    "class LatentODE1(Baseline):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, p, output_dim, batch_size,device):\n",
    "        Baseline.__init__(self,input_dim, hidden_dim, p, output_dim, device)\n",
    "        self.device = device\n",
    "        self.func = LatentODE1_(hidden_dim,input_dim,batch_size,device).to(device)\n",
    "        self.mu_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        ).to(device)\n",
    "        self.sigma_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Softplus(),\n",
    "        ).to(device)\n",
    "        self.encode_z0 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        ).to(device)\n",
    "        \n",
    "        for m in self.encode_z0.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "        \n",
    "    def forward(self, dt, x, p=0.0):\n",
    "        \n",
    "        #x = x.squeeze(0)\n",
    "        batch_size = x.size(0)\n",
    "        T = x.size(1)\n",
    "        \n",
    "        # ODE\n",
    "        mu_out = torch.zeros(batch_size,T,1,device = self.device)\n",
    "        sigma_out = torch.zeros(batch_size,T,1,device = self.device)\n",
    "        #z0 = 0.01 * torch.randn(batch_size,1,self.hidden_dim,device = self.device)\n",
    "        #z0 = self.encode_z0(x[:,0,:]).unsqueeze(1)\n",
    "        z0 = torch.zeros(batch_size,self.hidden_dim,device = self.device)\n",
    "        for i in range(0,T):\n",
    "            x_i = x[:,i,:]\n",
    "            dt_i = (dt[:,i,:][:,1] - dt[:,i,:][:,0]).unsqueeze(1).unsqueeze(1)\n",
    "            z0 = self.func.solve_ode(z0.clone(),dt_i,x_i)\n",
    "            mu_out[:,i:(i+1),:] = self.mu_net(z0.squeeze(1)).unsqueeze(1)\n",
    "            sigma_out[:,i:(i+1),:] = self.sigma_net(z0.squeeze(1)).unsqueeze(1)\n",
    "            \n",
    "        return mu_out,sigma_out\n",
    "    \n",
    "    def loss_fn(self,mu_s_,y,msk):\n",
    "        y_, s_ = mu_s_\n",
    "        distribution = torch.distributions.normal.Normal(y_[msk], s_[msk])\n",
    "        likelihood = distribution.log_prob(y[msk])\n",
    "        return -torch.mean(likelihood)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
