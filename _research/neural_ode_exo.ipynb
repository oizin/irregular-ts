{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5befb058",
   "metadata": {},
   "source": [
    "Things I've fixed:  \n",
    "\n",
    "* number of rows - pad at 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ad869",
   "metadata": {},
   "source": [
    "Some timings on the validation data (so multiply by 10)\n",
    "\n",
    "batch size: 56  \n",
    "CUDA - CPU times: user 2min 13s, sys: 1.11 s, total: 2min 14s  \n",
    "CPU - CPU times: user 1min 18s, sys: 3.22 s, total: 1min 21s  \n",
    "\n",
    "batch size: 128  \n",
    "CUDA - CPU times: user 1min 31s, sys: 651 ms, total: 1min 32s  \n",
    "CPU - CPU times: user 1min 18s, sys: 3.22 s, total: 1min 21s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c8010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import copy\n",
    "\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f88339",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "842a62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40baa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_loader import MIMICDataset,import_data\n",
    "from src.utils import setup_logger\n",
    "from src.training.training_nn import *\n",
    "from src.utils import seed_everything\n",
    "from src.data.data_scaler import PreProcess\n",
    "from data.feature_sets import all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098d3abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glc_transform(x):\n",
    "    x = x.copy()\n",
    "    x[x > 0] = np.log(x[x > 0]) - np.log(140)\n",
    "    return x\n",
    "\n",
    "def glc_invtransform(x):\n",
    "    x = x.copy()\n",
    "    x = np.exp(x + np.log(140))\n",
    "    return x\n",
    "\n",
    "ginv = glc_invtransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "774c3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_numpy(vec, pad):\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[0] = pad - vec.shape[0]\n",
    "    return np.concatenate([vec, np.zeros(pad_size)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1ff5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def import_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    ids = df.icustay_id.unique()\n",
    "    for id_ in ids:\n",
    "        df_id = df.loc[df.icustay_id == id_,:]\n",
    "        if (sum(df_id.msk) == df_id.shape[0]):\n",
    "            df.drop(df.loc[df.icustay_id == id_,:].index,inplace=True)\n",
    "            print(\"excluding:\",id_)\n",
    "    return df\n",
    "\n",
    "TIME_VARS = [\"timer\",\"timer_dt\"]\n",
    "\n",
    "def pad_numpy(vec, pad, val=\"zeros\"):\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[0] = pad - vec.shape[0]\n",
    "    if val == \"zeros\":\n",
    "        out = np.concatenate([vec, np.zeros(pad_size)], axis=0)\n",
    "    elif val == \"ones\":\n",
    "        out = np.concatenate([vec, np.ones(pad_size)], axis=0)\n",
    "    return out\n",
    "\n",
    "TIME_VARS = [\"timer\",\"timer_dt\"]\n",
    "\n",
    "class MIMICDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        patientunitstayids: \n",
    "        df:\n",
    "        ...\n",
    "    \n",
    "    Example:\n",
    "    \"\"\"\n",
    "    def __init__(self,df,features,pad=100,maxrows=100,verbose=True):\n",
    "        self.pad = pad\n",
    "        self.maxrows = maxrows\n",
    "        self.X,self.y,self.msk,self.dt,self.seqlen = self.load_data(df,features,verbose=verbose)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # pad\n",
    "        X = pad_numpy(self.X[idx],self.pad).astype(np.float32)\n",
    "        y = pad_numpy(self.y[idx],self.pad).astype(np.float32)\n",
    "        msk = pad_numpy(self.msk[idx],self.pad,val=\"ones\").astype(np.int32)\n",
    "        dt = pad_numpy(self.dt[idx],self.pad).astype(np.float32)\n",
    "        seqlen = self.seqlen[idx]\n",
    "\n",
    "        return X,y,msk,dt,seqlen\n",
    "    \n",
    "    def load_data(self,df,features,verbose):\n",
    "        excl = []\n",
    "        n_excl_pt = 0\n",
    "        n_excl_rws = 0\n",
    "        X_list, y_list, msk_list, dt_list, seqlen_list = [], [], [], [], []\n",
    "        ids = df.icustay_id.unique()\n",
    "        if verbose:\n",
    "            print(\"reconfiguring data...\")\n",
    "        for id_ in ids:\n",
    "            df_id = df.loc[df.icustay_id == id_,:].iloc[0:self.maxrows]\n",
    "            if (sum(df_id.msk) == df_id.shape[0]):\n",
    "                #print(\"excluding:\",id_)\n",
    "                n_excl_pt += 1\n",
    "                n_excl_rws +=  df_id.loc[df_id.msk == 0].shape[0]\n",
    "                continue\n",
    "            # additional exclusions\n",
    "#             if ((max(df_id.timer)/24 > 7.0) or (max(df_id.timer) < 6.0)):\n",
    "#                 print(\"excluding (time):\",id_)\n",
    "#                 continue\n",
    "            n_days_gl = (max(df_id.timer_dt) - min(df_id.timer))/24\n",
    "            n_gl_meas = df_id.loc[df_id.msk == 0].shape[0]\n",
    "            if (n_gl_meas/n_days_gl < 1.0):\n",
    "                n_excl_pt += 1\n",
    "                n_excl_rws +=  df_id.loc[df_id.msk == 0].shape[0]\n",
    "                #print(\"excluding (<1/day):\",id_)\n",
    "                continue\n",
    "            X = df_id.loc[:,features]\n",
    "            y = df_id.loc[:,\"glc_dt\"]\n",
    "            msk = df_id.loc[:,\"msk\"]\n",
    "            dt = df_id.loc[:,TIME_VARS]\n",
    "            seqlen = df_id.shape[0]\n",
    "            X = np.array(X).astype(np.float32)\n",
    "            y = np.array(y).astype(np.float32)\n",
    "            msk = np.array(msk).astype(np.int32)\n",
    "            dt = np.array(dt).astype(np.float32)\n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "            msk_list.append(msk)\n",
    "            dt_list.append(dt)\n",
    "            seqlen_list.append(seqlen)\n",
    "        print(\"excluded patients:\",n_excl_pt)\n",
    "        print(\"excluded rows:\",n_excl_rws)\n",
    "        return X_list,y_list,msk_list,dt_list,seqlen_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eba6c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "train_ids, valid_ids = train_test_split(df.icustay_id.unique(),test_size=0.1)\n",
    "df_train = df.loc[df.icustay_id.isin(train_ids)]\n",
    "df_valid = df.loc[df.icustay_id.isin(valid_ids)]\n",
    "FEATURES = all_features()\n",
    "NFEATURES = len(all_features())\n",
    "preproc = PreProcess(FEATURES,QuantileTransformer())\n",
    "preproc.fit(df_train)\n",
    "df_train = preproc.transform(df_train)\n",
    "df_valid = preproc.transform(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c346419a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.8632813011104"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(np.mean((ginv(df_train.loc[df_train.msk==0,'glc']) - ginv(df_train.loc[df_train.msk==0,'glc_dt']))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c15c18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.50852589919706"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(np.mean((ginv(df_valid.loc[df_valid.msk==0,'glc']) - ginv(df_valid.loc[df_valid.msk==0,'glc_dt']))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "987842b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconfiguring data...\n",
      "excluded patients: 235\n",
      "excluded rows: 659\n",
      "reconfiguring data...\n",
      "excluded patients: 16\n",
      "excluded rows: 43\n"
     ]
    }
   ],
   "source": [
    "dl_train = DataLoader(MIMICDataset(df_train,FEATURES),batch_size=128,pin_memory=False)\n",
    "dl_valid = DataLoader(MIMICDataset(df_valid,FEATURES),batch_size=128,pin_memory=False)\n",
    "dataloaders = {'train':dl_train,'validation':dl_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ba45e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df.groupby(['icustay_id']).size(),range=[0,200],bins=50)\n",
    "plt.xlabel('Number of observations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7b8d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from torchdiffeq import odeint_adjoint as odeint\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "import math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, p, output_dim, device):\n",
    "        super(Baseline, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.p = p\n",
    "        self.device = device\n",
    "        \n",
    "    def train_single_epoch(self,dataloader,optim):\n",
    "        loss = 0.0\n",
    "        n_batches = len(dataloader)\n",
    "        print(\"number of batchs: {}\".format(n_batches))\n",
    "        for i, (x, y, msk, dt, seqlen) in enumerate(dataloader):\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            dt = dt.to(self.device)\n",
    "            msk = msk.bool().to(self.device)\n",
    "            optim.zero_grad()\n",
    "            preds = self.forward(dt,x).squeeze(2)\n",
    "            loss_step = self.loss_fn(preds,y,~msk.squeeze(0))\n",
    "            loss_step.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), 10.0)\n",
    "            optim.step()\n",
    "            loss += loss_step.item()\n",
    "            if i % int(n_batches/4) == 0:\n",
    "                print(\"Batch number: {}\".format(i))\n",
    "                print(\"BATCH_loss : {:05.3f}\".format(loss_step.item()))\n",
    "        loss /= (i + 1)\n",
    "        print(\"EPOCH_loss : {:05.3f}\".format(loss))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def evaluate(self,dataloader,p=0.0):\n",
    "        # batch size should be 1\n",
    "        \n",
    "        rmse, loss = 0., 0.\n",
    "        N = 0\n",
    "        y_preds = []\n",
    "        y_tests = []\n",
    "        msks = []\n",
    "        #dts = []\n",
    "        with tqdm(total=len(dataloader)) as t:\n",
    "            for i, (x, y, msk, dt,seqlen) in enumerate(dataloader):\n",
    "                N += sum(sum(msk == 0)).item()\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                dt = dt.to(self.device)\n",
    "                # model prediction\n",
    "                y_ = self.forward(dt,x).squeeze(2)\n",
    "                y_preds.append([yc.detach().cpu().numpy() for yc in y_]) \n",
    "                y_tests.append(y.cpu().numpy())\n",
    "                msk = msk.bool().to(self.device)\n",
    "                rmse += self.get_sse(y_,y,~msk.squeeze(0)).item()\n",
    "                loss += self.loss_fn(y_,y,~msk.squeeze(0)).item()\n",
    "                msks.append(msk.cpu().numpy())\n",
    "                t.update()\n",
    "        rmse /= N\n",
    "        loss /= (i + 1)\n",
    "        rmse = math.sqrt(rmse)\n",
    "        print(\"_rmse : {:05.3f}\".format(rmse))\n",
    "        print(\"_loss : {:05.3f}\".format(loss))\n",
    "        return loss,rmse, y_preds, y_tests, msks\n",
    "\n",
    "    def get_sse(self,y_,y,msk):\n",
    "        \"\"\"\n",
    "        SSE: sum of squared errors\n",
    "        \"\"\"\n",
    "        if type(y_) == tuple:\n",
    "            y_ = y_[0]\n",
    "        c = torch.log(torch.tensor(140.0))\n",
    "        rmse = torch.sum((torch.exp(y_[msk] + c) - torch.exp(y[msk] + c))**2)\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "beb7ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"\n",
    "    dglucose/dt = NN(glucose,insulin)\n",
    "    \"\"\"\n",
    "    def __init__(self,input_dim,device):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        \n",
    "        self.x = torch.zeros(128,100,6).to(device)\n",
    "        self.dt = torch.zeros(128,100,1).to(device)\n",
    "        self.device = device\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        xz = torch.cat((z,self.x),2)\n",
    "        out = self.net(xz)\n",
    "        out = out * (self.dt/100) # -> scale by timestep\n",
    "        return out \n",
    "    \n",
    "    def solve_ode(self, x0, t, x):\n",
    "        self.x = x  # overwrites\n",
    "        self.dt = t\n",
    "        outputs = odeint(self, x0, torch.tensor([0,1.0]).to(self.device),rtol=1e-3, atol=1e-3)[1]\n",
    "        return outputs\n",
    "    \n",
    "class neuralODE(Baseline):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, p, output_dim, device):\n",
    "        Baseline.__init__(self,input_dim, hidden_dim, p, output_dim, device)\n",
    "        self.device = device\n",
    "        self.func = ODEFunc(input_dim,device).to(device)\n",
    "        \n",
    "    def forward(self, dt, x, seqlen=0):\n",
    "        \n",
    "        #x = x.squeeze(0)\n",
    "        #dt = dt.squeeze(0)\n",
    "        T = x.size(1)\n",
    "        \n",
    "        # ODE\n",
    "        mu_out = torch.zeros(x.size(0),T,1,device = self.device)\n",
    "        for i in range(0,T):\n",
    "            y0 = x[:,i:(i+1),0:1]\n",
    "            x_i = x[:,i:(i+1),1:]\n",
    "            dt_i = (dt[:,i,:][:,1] - dt[:,i,:][:,0]).unsqueeze(1).unsqueeze(1)\n",
    "            mu_out[:,i:(i+1),:] = self.func.solve_ode(y0,dt_i,x_i)\n",
    "\n",
    "        return mu_out\n",
    "    \n",
    "    def loss_fn(self,y_,y,msk):\n",
    "        return torch.mean((y_[msk] - y[msk])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d49b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dl_train = DataLoader(MIMICDataset(df_train,FEATURES),batch_size=2,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "967332cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "number of batchs: 86\n",
      "Batch number: 0\n",
      "BATCH_loss : 0.090\n",
      "Batch number: 21\n",
      "BATCH_loss : 0.070\n",
      "Batch number: 42\n",
      "BATCH_loss : 0.077\n",
      "Batch number: 63\n",
      "BATCH_loss : 0.070\n",
      "Batch number: 84\n",
      "BATCH_loss : 0.077\n",
      "EPOCH_loss : 0.073\n",
      "EPOCH: 1\n",
      "number of batchs: 86\n",
      "Batch number: 0\n",
      "BATCH_loss : 0.084\n",
      "Batch number: 21\n",
      "BATCH_loss : 0.068\n",
      "Batch number: 42\n",
      "BATCH_loss : 0.076\n",
      "Batch number: 63\n",
      "BATCH_loss : 0.068\n",
      "Batch number: 84\n",
      "BATCH_loss : 0.076\n",
      "EPOCH_loss : 0.071\n",
      "256.4110782146454\n"
     ]
    }
   ],
   "source": [
    "nepochs = 2\n",
    "n_ode = neuralODE(NFEATURES,1,0.0,1,\"cuda\").to(\"cuda\")\n",
    "model_optim = optim.Adam(n_ode.parameters(), lr=1e-2)\n",
    "start_time = time.time()\n",
    "for epoch in range(0,nepochs):\n",
    "    print(\"EPOCH: {}\".format(epoch))\n",
    "    n_ode.train_single_epoch(dl_train,model_optim)\n",
    "current_time = time.time()\n",
    "elapsed_time = current_time - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e610c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate - averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6b108569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_rmse : 42.164\n",
      "_loss : 0.069\n",
      "42.16436924932725\n",
      "0.06897715628147125\n",
      "0.06976147\n",
      "0.3493343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss,rmse, y_preds, y_tests, msks = n_ode.evaluate(dl_valid)\n",
    "print(rmse)\n",
    "print(loss)\n",
    "\n",
    "y_preds = (np.concatenate(y_preds))\n",
    "y_tests = (np.concatenate(y_tests))\n",
    "msk = (np.concatenate(msks))\n",
    "\n",
    "diff = y_preds - y_tests\n",
    "print(np.mean(diff[~msk]**2))\n",
    "print(np.std(y_tests[~msk]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a103706",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_tests[~msk],bins=20)\n",
    "plt.show()\n",
    "plt.hist(y_preds[~msk],bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a8bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_preds[~msk],y_tests[~msk])\n",
    "plt.show()\n",
    "c = np.log(140)\n",
    "plt.scatter(ginv(y_preds[~msk]),ginv(y_tests[~msk]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9346baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "res = np.zeros(N)\n",
    "Y0 = torch.linspace(-1.0,1.0,N)\n",
    "y0 = torch.zeros(1,1,1)\n",
    "dt = torch.ones(1,1,1).to(\"cuda\")\n",
    "n_ode.func.x = torch.zeros(1,1,1).to(\"cuda\")\n",
    "n_ode.func.dt = dt\n",
    "for i in range(0,N):\n",
    "    y0[0,0,0] = Y0[i]\n",
    "    with torch.no_grad():\n",
    "        res[i] = n_ode.func(dt,y0.to(\"cuda\")).cpu().numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0 = Y0.cpu().numpy()\n",
    "plt.scatter(Y0,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y0,Y0 + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.linspace(0,1,10)\n",
    "fig, ax = plt.subplots(1, 1, tight_layout=True)\n",
    "for t in ts:\n",
    "    for i in range(N):\n",
    "        dy =  np.exp(Y0[i]+res[i]+np.log(140)) - np.exp(Y0[i]+np.log(140))\n",
    "        ax.arrow(t,np.exp(Y0[i]+np.log(140)),0.09,dy, width = 0.008)\n",
    "ax.set_xlabel(\"Time (hours)\")\n",
    "ax.set_ylabel(\"Blood glucose (mg/dL)\")\n",
    "fig.savefig('tmp.jpg', bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca166e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "14fcc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LODEFunc(nn.Module):\n",
    "    \"\"\"\n",
    "    dglucose/dt = NN(glucose,insulin)\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_dim,input_dim,device):\n",
    "        super(LODEFunc, self).__init__()\n",
    "\n",
    "        self.x = torch.zeros(128,100,input_dim).to(device)\n",
    "        self.dt = torch.zeros(128,100,1).to(device)\n",
    "        self.device = device\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim+hidden_dim, hidden_dim * 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        xz = torch.cat((z,self.x),2)\n",
    "        out = self.net(xz)\n",
    "        out = out * (self.dt/100) # -> scale by timestep\n",
    "        return out \n",
    "\n",
    "    def solve_ode(self, z0, t, x):\n",
    "        self.x = x  # overwrites\n",
    "        self.dt = t\n",
    "        outputs = odeint(self, z0, torch.tensor([0,1.0]).to(self.device),rtol=1e-1, atol=1e-1)[1]\n",
    "        return outputs\n",
    "    \n",
    "class latentODE(Baseline):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, p, output_dim, device):\n",
    "        Baseline.__init__(self,input_dim, hidden_dim, p, output_dim, device)\n",
    "        self.device = device\n",
    "        self.func = LODEFunc(hidden_dim,input_dim,device).to(device)\n",
    "        self.mu_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        ).to(device)\n",
    "        self.encode_z0 = nn.Sequential(\n",
    "            nn.Linear(input_dim, max(hidden_dim,hidden_dim) // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(max(hidden_dim,hidden_dim) // 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        ).to(device)\n",
    "        \n",
    "        for m in self.encode_z0.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "        \n",
    "    def forward(self, dt, x, seqlen=0):\n",
    "        \n",
    "        #x = x.squeeze(0)\n",
    "        batch_size = x.size(0)\n",
    "        T = x.size(1)\n",
    "        \n",
    "        # ODE\n",
    "        mu_out = torch.zeros(batch_size,T,1,device = self.device)\n",
    "        #z0 = 0.01 * torch.randn(batch_size,1,self.hidden_dim,device = self.device)\n",
    "        z0 = self.encode_z0(x[:,0,:]).unsqueeze(1)\n",
    "        for i in range(0,T):\n",
    "            x_i = x[:,i:(i+1),:]\n",
    "            dt_i = (dt[:,i,:][:,1] - dt[:,i,:][:,0]).unsqueeze(1).unsqueeze(1)\n",
    "            z0 = self.func.solve_ode(z0,dt_i,x_i)\n",
    "            mu_out[:,i:(i+1),:] = self.mu_net(z0.squeeze(1)).unsqueeze(1)\n",
    "\n",
    "        return mu_out\n",
    "    \n",
    "    def loss_fn(self,y_,y,msk):\n",
    "        return torch.mean((y_[msk] - y[msk])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a833ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 100, 1])\n",
      "tensor(0.1486, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "l_ode = latentODE(NFEATURES,8,0.0,1,\"cuda\").to(\"cuda\")\n",
    "x,y,msk,dt,_ = next(iter(dl_train))\n",
    "y_ = l_ode(dt.to(\"cuda\"),x.to(\"cuda\"))\n",
    "print(y_.shape)\n",
    "loss = l_ode.loss_fn(y.to(\"cuda\"),y_.squeeze(2),~msk.bool().to(\"cuda\").squeeze(0))\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3f336a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "number of batchs: 86\n",
      "Batch number: 0\n",
      "BATCH_loss : 0.238\n",
      "Batch number: 21\n",
      "BATCH_loss : 0.112\n",
      "Batch number: 42\n",
      "BATCH_loss : 0.101\n",
      "Batch number: 63\n",
      "BATCH_loss : 0.107\n",
      "Batch number: 84\n",
      "BATCH_loss : 0.104\n",
      "EPOCH_loss : 0.108\n",
      "EPOCH: 1\n",
      "number of batchs: 86\n",
      "Batch number: 0\n",
      "BATCH_loss : 0.098\n",
      "Batch number: 21\n",
      "BATCH_loss : 0.081\n",
      "Batch number: 42\n",
      "BATCH_loss : 0.082\n",
      "Batch number: 63\n",
      "BATCH_loss : 0.092\n",
      "Batch number: 84\n",
      "BATCH_loss : 0.099\n",
      "EPOCH_loss : 0.087\n",
      "EPOCH: 2\n",
      "number of batchs: 86\n",
      "Batch number: 0\n",
      "BATCH_loss : 0.097\n",
      "Batch number: 21\n",
      "BATCH_loss : 0.078\n",
      "Batch number: 42\n",
      "BATCH_loss : 0.077\n",
      "Batch number: 63\n",
      "BATCH_loss : 0.085\n",
      "Batch number: 84\n",
      "BATCH_loss : 0.092\n",
      "EPOCH_loss : 0.082\n",
      "EPOCH: 3\n",
      "number of batchs: 86\n",
      "Batch number: 0\n",
      "BATCH_loss : 0.091\n",
      "Batch number: 21\n",
      "BATCH_loss : 0.077\n",
      "Batch number: 42\n",
      "BATCH_loss : 0.075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30965/389348401.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPOCH: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0ml_ode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_optim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcurrent_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30965/2470841599.py\u001b[0m in \u001b[0;36mtrain_single_epoch\u001b[0;34m(self, dataloader, optim)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mmsk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mloss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmsk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30965/2243593284.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dt, x, seqlen)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mdt_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mz0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_ode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdt_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mmu_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmu_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSiLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nepochs = 5\n",
    "l_ode = latentODE(NFEATURES,10,0.1,1,\"cuda\")\n",
    "model_optim = optim.Adam(l_ode.parameters(), lr=1e-2)\n",
    "start_time = time.time()\n",
    "for epoch in range(0,nepochs):\n",
    "    print(\"EPOCH: {}\".format(epoch))\n",
    "    l_ode.train_single_epoch(dl_train,model_optim)\n",
    "current_time = time.time()\n",
    "elapsed_time = current_time - start_time\n",
    "print(\"time: {}\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c19c12ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_rmse : 51.927\n",
      "_loss : 0.090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss,rmse, y_preds, y_tests, msks = l_ode.evaluate(dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = (np.concatenate(y_preds))\n",
    "y_tests = (np.concatenate(y_tests))\n",
    "msk = (np.concatenate(msks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23cb231",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ginv(y_preds[~msk]),ginv(y_tests[~msk]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee787d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear system of z = (g,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "487f71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    " class recurrentODE_(nn.Module):\n",
    "    \"\"\"\n",
    "    (dglucose/dt,dh/dt) = (RNN(glucose,h),NN(h,x))\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_dim,input_dim,batch_size,device):\n",
    "        super(recurrentODE_, self).__init__()\n",
    "        \n",
    "        self.x = torch.zeros(batch_size,100,input_dim).to(device)\n",
    "        self.dt = torch.zeros(batch_size,100,1).to(device)\n",
    "        self.device = device\n",
    "        # dh/dt\n",
    "        self.dh = nn.Sequential(\n",
    "            nn.Linear(hidden_dim+input_dim, (hidden_dim+input_dim)*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear((hidden_dim+input_dim)*2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        ).to(device)\n",
    "        # dy/dt\n",
    "        self.dy_rnn = nn.RNNCell(input_dim, hidden_dim)\n",
    "        self.dy_lin = nn.Linear(hidden_dim, 1)\n",
    "        self.dy_tanh = nn.Tanh()\n",
    "\n",
    "        for m in self.dh.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "        nn.init.normal_(self.dy_rnn.weight_ih,mean=0, std=0.1)\n",
    "        nn.init.normal_(self.dy_rnn.weight_hh,mean=0, std=0.1)\n",
    "        nn.init.constant_(self.dy_rnn.bias_ih, val=0)\n",
    "        nn.init.constant_(self.dy_rnn.bias_ih, val=0)\n",
    "        nn.init.normal_(self.dy_lin.weight,mean=0, std=0.1)\n",
    "        nn.init.constant_(self.dy_lin.bias, val=0)\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        # dh/dt\n",
    "        #dh = self.dh(z[:,:,1:])*self.dt\n",
    "        dh = self.dh(torch.cat((self.x,z[:,:,1:]),2)) * (self.dt / 50)\n",
    "        # dy/dt\n",
    "        #dy = self.dy_rnn(self.x.squeeze(1),z[:,:,1:].squeeze(1)) # hidden should be shape (1,batch,hidden)\n",
    "        #dy = self.dy_lin(dy.unsqueeze(1))\n",
    "        dy = self.dy_lin(z[:,:,1:])\n",
    "        dy = self.dy_tanh(dy) * (self.dt / 50)\n",
    "        return torch.cat((dy,dh),2)\n",
    "    \n",
    "    def solve_ode(self, z0, t, x):\n",
    "        self.x = x  # overwrites\n",
    "        self.dt = t\n",
    "        outputs = odeint(self, z0, torch.tensor([0,1.0]).to(self.device),rtol=1e-1, atol=1e-1)[1]\n",
    "        return outputs\n",
    "    \n",
    "class recurrentODE(Baseline):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, p, output_dim,batch_size, device):\n",
    "        Baseline.__init__(self,input_dim, hidden_dim, p, output_dim, device)\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.func = recurrentODE_(hidden_dim,input_dim,batch_size,device).to(device)\n",
    "        \n",
    "    def forward(self, dt, x, seqlen=0):\n",
    "        \n",
    "        #x = x.squeeze(0)\n",
    "        batch_size = x.size(0)\n",
    "        T = x.size(1)\n",
    "\n",
    "        # ODE stuff\n",
    "        mu_out = torch.zeros(batch_size,T,1,device = self.device)\n",
    "        z0 = torch.zeros(batch_size,1,self.hidden_dim+1,device = self.device)\n",
    "        for i in range(0,T):\n",
    "            x_i = x[:,i:(i+1),:]\n",
    "            y0 = x[:,i:(i+1),0:1]\n",
    "            z0[:,:,0:1] = y0\n",
    "            dt_i = (dt[:,i,:][:,1] - dt[:,i,:][:,0]).unsqueeze(1).unsqueeze(1)\n",
    "            z0 = self.func.solve_ode(z0,dt_i,x_i)\n",
    "            mu_out[:,i:(i+1),:] = z0[:,:,0:1]\n",
    "\n",
    "        return mu_out\n",
    "    \n",
    "    def loss_fn(self,y_,y,msk):\n",
    "        return torch.mean((y_[msk] - y[msk])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "da5fbb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "number of batchs: 86\n",
      "Batch number: 0\n",
      "BATCH_loss : 0.090\n",
      "Batch number: 21\n",
      "BATCH_loss : 0.072\n",
      "Batch number: 42\n",
      "BATCH_loss : 0.082\n",
      "Batch number: 63\n",
      "BATCH_loss : 0.074\n",
      "Batch number: 84\n",
      "BATCH_loss : 0.082\n",
      "EPOCH_loss : 0.076\n",
      "EPOCH: 1\n",
      "number of batchs: 86\n",
      "Batch number: 0\n",
      "BATCH_loss : 0.090\n",
      "Batch number: 21\n",
      "BATCH_loss : 0.072\n",
      "Batch number: 42\n",
      "BATCH_loss : 0.082\n",
      "Batch number: 63\n",
      "BATCH_loss : 0.074\n",
      "Batch number: 84\n",
      "BATCH_loss : 0.081\n",
      "EPOCH_loss : 0.076\n",
      "EPOCH: 2\n",
      "number of batchs: 86\n",
      "Batch number: 0\n",
      "BATCH_loss : 0.090\n",
      "Batch number: 21\n",
      "BATCH_loss : 0.072\n",
      "Batch number: 42\n",
      "BATCH_loss : 0.082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30965/2074084228.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPOCH: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mr_ode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_optim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mcurrent_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30965/2470841599.py\u001b[0m in \u001b[0;36mtrain_single_epoch\u001b[0;34m(self, dataloader, optim)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmsk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nepochs = 5\n",
    "batch_size = 128\n",
    "r_ode = recurrentODE(NFEATURES,4,0.0,1,batch_size,\"cuda\").to(\"cuda\")\n",
    "model_optim = optim.Adam(r_ode.parameters(), lr=1e-2)\n",
    "start_time = time.time()\n",
    "for epoch in range(0,nepochs):\n",
    "    print(\"EPOCH: {}\".format(epoch))\n",
    "    r_ode.train_single_epoch(dl_train,model_optim)\n",
    "current_time = time.time()\n",
    "elapsed_time = current_time - start_time\n",
    "print(\"time: {}\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6d23d5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_rmse : 43.812\n",
      "_loss : 0.074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss,rmse, y_preds, y_tests, msks = r_ode.evaluate(dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a48d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_rmse : 41.815\n",
    "_loss : 0.068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c393e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODE-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a90af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODERNN_(nn.Module):\n",
    "    \"\"\"\n",
    "    In an ODE-RNN the hidden state h_t of the RNN evolves according to\n",
    "    an ODE. This ODE is a neural network, i.e. dh/dt = ODEFunc(h,x).\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_dim,device):\n",
    "        super(ODERNN_, self).__init__()\n",
    "\n",
    "        self.x = torch.zeros(128,100,6).to(device)\n",
    "        self.dt = torch.zeros(128,100,1).to(device)\n",
    "        self.device = device\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, dt, y):\n",
    "        out = self.net(y)\n",
    "        out = out * (self.dt / 100)\n",
    "        return out\n",
    "    \n",
    "    def solve_ode(self, z0, t, x):\n",
    "        self.x = x  # overwrites\n",
    "        self.dt = t\n",
    "        outputs = odeint(self, z0, torch.tensor([0,1.0]).to(self.device),rtol=1e-1, atol=1e-1)[1]\n",
    "        return outputs\n",
    "\n",
    "class ODERNN(Baseline):\n",
    "    \"\"\"\n",
    "    ODE-RNN\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, p, output_dim, device):\n",
    "        Baseline.__init__(self,input_dim, hidden_dim, p, output_dim, device)\n",
    "        # ODE-RNN\n",
    "        self.rnn = nn.RNNCell(input_dim, hidden_dim)\n",
    "        self.func = ODERNN_(hidden_dim,device)\n",
    "        # N(mu,sigma)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l1 = nn.Linear(hidden_dim,hidden_dim//2)\n",
    "        self.distribution_mu = nn.Linear(hidden_dim//2, 1)\n",
    "\n",
    "    def forward(self, dt, x, p=0.0):\n",
    "        \n",
    "        T = x.size(1)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        mu_out = torch.zeros(batch_size,T,1,device = self.device)\n",
    "        h_t = torch.zeros(batch_size, self.rnn.hidden_size,device=self.device)\n",
    "        for i in range(0,T):\n",
    "            x_i = x[:,i:(i+1),:]\n",
    "            dt_i = (dt[:,i,:][:,1] - dt[:,i,:][:,0]).unsqueeze(1)\n",
    "            h_t = self.rnn(x_i.squeeze(1),h_t)\n",
    "            h_t = self.func.solve_ode(h_t,dt_i,x_i)\n",
    "            mu = self.l1(h_t)\n",
    "            mu = F.dropout(mu,training=True,p=p)\n",
    "            mu = self.relu(mu)\n",
    "            mu_out[:,i:(i+1),:] = self.distribution_mu(mu).unsqueeze(1)\n",
    "\n",
    "        return mu_out\n",
    "    \n",
    "    def loss_fn(self,y_,y,msk):\n",
    "        return torch.mean((y_[msk] - y[msk])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 5\n",
    "rnn_ode = ODERNN(NFEATURES,6,0.1,1,\"cuda\").to(\"cuda\")\n",
    "model_optim = optim.Adam(rnn_ode.parameters(), lr=1e-2)\n",
    "start_time = time.time()\n",
    "for epoch in range(0,nepochs):\n",
    "    print(\"EPOCH: {}\".format(epoch))\n",
    "    rnn_ode.train_single_epoch(dl_train,model_optim)\n",
    "current_time = time.time()\n",
    "elapsed_time = current_time - start_time\n",
    "print(\"time: {}\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea718ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,rmse, y_preds, y_tests, msks = rnn_ode.evaluate(dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_rmse : 43.876\n",
    "_loss : 0.062\n",
    "\n",
    "_rmse : 53.869\n",
    "_loss : 0.092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f621a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,msk,dt,_ = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22efacc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([67.,  9., 37.,  4.,  5.,  1.,  0.,  4.,  0.,  1.]),\n",
       " array([0.        , 0.04828   , 0.09655999, 0.14483999, 0.19311999,\n",
       "        0.24139999, 0.28967997, 0.33795998, 0.38623998, 0.43451998,\n",
       "        0.48279998], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist((dt[:,3,:][:,1] - dt[:,3,:][:,0]).unsqueeze(1).numpy()/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5673fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01fd910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
